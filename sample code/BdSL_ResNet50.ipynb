{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BdSL ResNet50.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAb77yZ9fzMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39f25ac-b4ca-4eb8-f231-55fdd398bda0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafL7Li0jyXW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfI1LfTvFXR6"
      },
      "source": [
        "train_dir = '/content/gdrive/MyDrive/Research and Project Stuffs/dataset Rafi/RESIZED_DATASET'\n",
        "validation_dir = '/content/gdrive/MyDrive/Research and Project Stuffs/dataset Rafi/RESIZED_TESTING_DATA'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXlO9rfqFbOA",
        "outputId": "23db1b9c-7184-43c8-88d4-3cf633869104"
      },
      "source": [
        "cd '/content/gdrive/MyDrive/Research and Project Stuffs/dataset Rafi'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Research and Project Stuffs/dataset Rafi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtptVJuQFmUg",
        "outputId": "b349e54f-e3fe-4070-e1f0-72b9427ac251"
      },
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "image_size = 224\n",
        "vgg_conv = ResNet50(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df-FHOFWFvod"
      },
      "source": [
        "# Freeze all the layers\n",
        "for layer in vgg_conv.layers[:]:\n",
        "    layer.trainable = False\n",
        "# Check the trainable status of the individual layers\n",
        "for layer in vgg_conv.layers:\n",
        "    print(layer, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iO7GUXHF1Ul",
        "outputId": "8f9d1be8-01d2-454e-8070-f9434b1c8fe4"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "#from tensorflow.keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Embedding, Dense, Dropout, Flatten, Input,GlobalAveragePooling2D\n",
        "from keras import activations\n",
        "#from tensorflow.python.keras.layers.core import Dense, Dropout, Flatten\n",
        "#from tensorflow.python.keras.layers import Input\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.layers import LSTM\n",
        "#from keras.layers import Dropout\n",
        "#from keras.applications.vgg16 import VGG16\n",
        "#from tensorflow.keras.applications import vgg16\n",
        "from skimage.color import gray2rgb\n",
        "#from tensorflow.keras.applications.inception_v3 import InceptionV3 \n",
        "#from tensorflow.keras.applications import DenseNet201\n",
        "# Create the model\n",
        "model_resnet = Sequential()\n",
        "# Add the vgg convolutional base model\n",
        "model_resnet.add(vgg_conv)\n",
        "# Add new layers\n",
        "model_resnet.add(Flatten())\n",
        "model_resnet.add(Dense(512, activation='relu'))\n",
        "model_resnet.add(Dropout(0.5))\n",
        "model_resnet.add(Dense(38, activation='softmax'))\n",
        "# Show a summary of the model. Check the number of trainable parameters\n",
        "model_resnet.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 100352)            0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               51380736  \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 38)                19494     \n",
            "=================================================================\n",
            "Total params: 74,987,942\n",
            "Trainable params: 51,400,230\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j-FKGM1GA4F",
        "outputId": "b2add984-cb4a-4cb2-9d96-c1ef77cd0638"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# Load the normalized images\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Change the batchsize according to your system RAM\n",
        "train_batchsize = 128\n",
        "val_batchsize = 10\n",
        "\n",
        "# Data generator for training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=train_batchsize,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Data generator for validation data\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=val_batchsize,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 11061 images belonging to 38 classes.\n",
            "Found 1520 images belonging to 38 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCVGMmK3GPGF",
        "outputId": "a30561e3-408a-4d1b-ed17-831779111a02"
      },
      "source": [
        "import keras\n",
        "sgd = keras.optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "# Configure the model for training\n",
        "model_resnet.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['acc'])\n",
        "\n",
        "# Train the model\n",
        "history = model_resnet.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=\n",
        "         train_generator.samples/train_generator.batch_size,\n",
        "      epochs=15,\n",
        "      validation_data=validation_generator, \n",
        "      validation_steps=\n",
        "         validation_generator.samples/validation_generator.batch_size,\n",
        "      verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "86/86 [==============================] - 47s 511ms/step - loss: 4.1034 - acc: 0.0267 - val_loss: 3.6334 - val_acc: 0.0309\n",
            "Epoch 2/15\n",
            "86/86 [==============================] - 43s 500ms/step - loss: 3.6382 - acc: 0.0300 - val_loss: 3.6261 - val_acc: 0.0421\n",
            "Epoch 3/15\n",
            "86/86 [==============================] - 43s 492ms/step - loss: 3.6287 - acc: 0.0351 - val_loss: 3.6228 - val_acc: 0.0329\n",
            "Epoch 4/15\n",
            "86/86 [==============================] - 44s 507ms/step - loss: 3.6218 - acc: 0.0377 - val_loss: 3.6213 - val_acc: 0.0441\n",
            "Epoch 5/15\n",
            "86/86 [==============================] - 42s 489ms/step - loss: 3.6236 - acc: 0.0396 - val_loss: 3.6207 - val_acc: 0.0461\n",
            "Epoch 6/15\n",
            "86/86 [==============================] - 44s 509ms/step - loss: 3.6192 - acc: 0.0414 - val_loss: 3.6177 - val_acc: 0.0395\n",
            "Epoch 7/15\n",
            "86/86 [==============================] - 44s 505ms/step - loss: 3.6190 - acc: 0.0432 - val_loss: 3.6182 - val_acc: 0.0368\n",
            "Epoch 8/15\n",
            "86/86 [==============================] - 42s 483ms/step - loss: 3.6188 - acc: 0.0382 - val_loss: 3.6157 - val_acc: 0.0592\n",
            "Epoch 9/15\n",
            "86/86 [==============================] - 42s 485ms/step - loss: 3.6191 - acc: 0.0374 - val_loss: 3.6136 - val_acc: 0.0724\n",
            "Epoch 10/15\n",
            "86/86 [==============================] - 44s 511ms/step - loss: 3.6162 - acc: 0.0414 - val_loss: 3.6156 - val_acc: 0.0421\n",
            "Epoch 11/15\n",
            "86/86 [==============================] - 42s 487ms/step - loss: 3.6148 - acc: 0.0455 - val_loss: 3.6114 - val_acc: 0.0428\n",
            "Epoch 12/15\n",
            "86/86 [==============================] - 44s 512ms/step - loss: 3.6139 - acc: 0.0383 - val_loss: 3.6104 - val_acc: 0.0612\n",
            "Epoch 13/15\n",
            "86/86 [==============================] - 45s 514ms/step - loss: 3.6070 - acc: 0.0483 - val_loss: 3.6082 - val_acc: 0.0671\n",
            "Epoch 14/15\n",
            "86/86 [==============================] - 43s 495ms/step - loss: 3.6095 - acc: 0.0463 - val_loss: 3.6072 - val_acc: 0.0717\n",
            "Epoch 15/15\n",
            "86/86 [==============================] - 45s 525ms/step - loss: 3.6106 - acc: 0.0459 - val_loss: 3.6038 - val_acc: 0.0717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMr5xBDnGbl8"
      },
      "source": [
        "del model_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lam5YZCbMAAQ",
        "outputId": "62317b42-c723-4c71-d7c8-1f67a470a7e1"
      },
      "source": [
        "history = model_resnet.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=\n",
        "         train_generator.samples/train_generator.batch_size,\n",
        "      epochs=200,\n",
        "      validation_data=validation_generator, \n",
        "      validation_steps=\n",
        "         validation_generator.samples/validation_generator.batch_size,\n",
        "      verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "86/86 [==============================] - 43s 496ms/step - loss: 3.6052 - acc: 0.0534 - val_loss: 3.6023 - val_acc: 0.0658\n",
            "Epoch 2/200\n",
            "86/86 [==============================] - 44s 504ms/step - loss: 3.6048 - acc: 0.0488 - val_loss: 3.6021 - val_acc: 0.0743\n",
            "Epoch 3/200\n",
            "86/86 [==============================] - 44s 507ms/step - loss: 3.6052 - acc: 0.0466 - val_loss: 3.6011 - val_acc: 0.0783\n",
            "Epoch 4/200\n",
            "86/86 [==============================] - 46s 530ms/step - loss: 3.6027 - acc: 0.0563 - val_loss: 3.5977 - val_acc: 0.0908\n",
            "Epoch 5/200\n",
            "86/86 [==============================] - 44s 502ms/step - loss: 3.5997 - acc: 0.0565 - val_loss: 3.5965 - val_acc: 0.0743\n",
            "Epoch 6/200\n",
            "86/86 [==============================] - 44s 511ms/step - loss: 3.5982 - acc: 0.0534 - val_loss: 3.5928 - val_acc: 0.0770\n",
            "Epoch 7/200\n",
            "86/86 [==============================] - 41s 475ms/step - loss: 3.5967 - acc: 0.0545 - val_loss: 3.5916 - val_acc: 0.0770\n",
            "Epoch 8/200\n",
            "86/86 [==============================] - 42s 485ms/step - loss: 3.5935 - acc: 0.0591 - val_loss: 3.5888 - val_acc: 0.0697\n",
            "Epoch 9/200\n",
            "86/86 [==============================] - 41s 475ms/step - loss: 3.5948 - acc: 0.0611 - val_loss: 3.5881 - val_acc: 0.0836\n",
            "Epoch 10/200\n",
            "86/86 [==============================] - 44s 512ms/step - loss: 3.5916 - acc: 0.0571 - val_loss: 3.5885 - val_acc: 0.0618\n",
            "Epoch 11/200\n",
            "86/86 [==============================] - 42s 487ms/step - loss: 3.5899 - acc: 0.0598 - val_loss: 3.5843 - val_acc: 0.0842\n",
            "Epoch 12/200\n",
            "86/86 [==============================] - 44s 508ms/step - loss: 3.5872 - acc: 0.0612 - val_loss: 3.5832 - val_acc: 0.0842\n",
            "Epoch 13/200\n",
            "86/86 [==============================] - 43s 495ms/step - loss: 3.5865 - acc: 0.0655 - val_loss: 3.5787 - val_acc: 0.0993\n",
            "Epoch 14/200\n",
            "86/86 [==============================] - 42s 481ms/step - loss: 3.5829 - acc: 0.0634 - val_loss: 3.5810 - val_acc: 0.0987\n",
            "Epoch 15/200\n",
            "86/86 [==============================] - 43s 499ms/step - loss: 3.5837 - acc: 0.0633 - val_loss: 3.5797 - val_acc: 0.0671\n",
            "Epoch 16/200\n",
            "86/86 [==============================] - 43s 501ms/step - loss: 3.5803 - acc: 0.0657 - val_loss: 3.5736 - val_acc: 0.1158\n",
            "Epoch 17/200\n",
            "86/86 [==============================] - 43s 501ms/step - loss: 3.5783 - acc: 0.0718 - val_loss: 3.5737 - val_acc: 0.1026\n",
            "Epoch 18/200\n",
            "86/86 [==============================] - 44s 507ms/step - loss: 3.5766 - acc: 0.0688 - val_loss: 3.5700 - val_acc: 0.1079\n",
            "Epoch 19/200\n",
            "86/86 [==============================] - 46s 534ms/step - loss: 3.5753 - acc: 0.0673 - val_loss: 3.5710 - val_acc: 0.0928\n",
            "Epoch 20/200\n",
            "86/86 [==============================] - 44s 508ms/step - loss: 3.5732 - acc: 0.0697 - val_loss: 3.5688 - val_acc: 0.1237\n",
            "Epoch 21/200\n",
            "86/86 [==============================] - 45s 521ms/step - loss: 3.5708 - acc: 0.0730 - val_loss: 3.5678 - val_acc: 0.1020\n",
            "Epoch 22/200\n",
            "86/86 [==============================] - 43s 493ms/step - loss: 3.5713 - acc: 0.0683 - val_loss: 3.5678 - val_acc: 0.1026\n",
            "Epoch 23/200\n",
            "86/86 [==============================] - 43s 491ms/step - loss: 3.5708 - acc: 0.0700 - val_loss: 3.5628 - val_acc: 0.1059\n",
            "Epoch 24/200\n",
            "86/86 [==============================] - 46s 530ms/step - loss: 3.5645 - acc: 0.0767 - val_loss: 3.5600 - val_acc: 0.1079\n",
            "Epoch 25/200\n",
            "86/86 [==============================] - 44s 510ms/step - loss: 3.5631 - acc: 0.0723 - val_loss: 3.5581 - val_acc: 0.1197\n",
            "Epoch 26/200\n",
            "86/86 [==============================] - 45s 514ms/step - loss: 3.5619 - acc: 0.0729 - val_loss: 3.5569 - val_acc: 0.0934\n",
            "Epoch 27/200\n",
            "86/86 [==============================] - 41s 476ms/step - loss: 3.5619 - acc: 0.0735 - val_loss: 3.5580 - val_acc: 0.0875\n",
            "Epoch 28/200\n",
            "86/86 [==============================] - 43s 499ms/step - loss: 3.5562 - acc: 0.0806 - val_loss: 3.5523 - val_acc: 0.1217\n",
            "Epoch 29/200\n",
            "86/86 [==============================] - 43s 499ms/step - loss: 3.5583 - acc: 0.0758 - val_loss: 3.5523 - val_acc: 0.1026\n",
            "Epoch 30/200\n",
            "86/86 [==============================] - 42s 484ms/step - loss: 3.5557 - acc: 0.0768 - val_loss: 3.5512 - val_acc: 0.1368\n",
            "Epoch 31/200\n",
            "86/86 [==============================] - 45s 516ms/step - loss: 3.5553 - acc: 0.0796 - val_loss: 3.5484 - val_acc: 0.1257\n",
            "Epoch 32/200\n",
            "86/86 [==============================] - 43s 500ms/step - loss: 3.5525 - acc: 0.0781 - val_loss: 3.5466 - val_acc: 0.1099\n",
            "Epoch 33/200\n",
            "86/86 [==============================] - 44s 512ms/step - loss: 3.5482 - acc: 0.0818 - val_loss: 3.5450 - val_acc: 0.1105\n",
            "Epoch 34/200\n",
            "86/86 [==============================] - 43s 499ms/step - loss: 3.5511 - acc: 0.0779 - val_loss: 3.5439 - val_acc: 0.1263\n",
            "Epoch 35/200\n",
            "86/86 [==============================] - 44s 513ms/step - loss: 3.5456 - acc: 0.0840 - val_loss: 3.5416 - val_acc: 0.1007\n",
            "Epoch 36/200\n",
            "86/86 [==============================] - 45s 514ms/step - loss: 3.5443 - acc: 0.0812 - val_loss: 3.5414 - val_acc: 0.1020\n",
            "Epoch 37/200\n",
            "86/86 [==============================] - 44s 506ms/step - loss: 3.5431 - acc: 0.0830 - val_loss: 3.5361 - val_acc: 0.1184\n",
            "Epoch 38/200\n",
            "86/86 [==============================] - 42s 490ms/step - loss: 3.5393 - acc: 0.0890 - val_loss: 3.5349 - val_acc: 0.1368\n",
            "Epoch 39/200\n",
            "86/86 [==============================] - 44s 509ms/step - loss: 3.5415 - acc: 0.0816 - val_loss: 3.5346 - val_acc: 0.1289\n",
            "Epoch 40/200\n",
            "86/86 [==============================] - 45s 520ms/step - loss: 3.5378 - acc: 0.0853 - val_loss: 3.5312 - val_acc: 0.1303\n",
            "Epoch 41/200\n",
            "86/86 [==============================] - 42s 485ms/step - loss: 3.5361 - acc: 0.0840 - val_loss: 3.5304 - val_acc: 0.1145\n",
            "Epoch 42/200\n",
            "86/86 [==============================] - 44s 514ms/step - loss: 3.5328 - acc: 0.0896 - val_loss: 3.5314 - val_acc: 0.1191\n",
            "Epoch 43/200\n",
            "86/86 [==============================] - 42s 486ms/step - loss: 3.5316 - acc: 0.0900 - val_loss: 3.5270 - val_acc: 0.0987\n",
            "Epoch 44/200\n",
            "86/86 [==============================] - 41s 475ms/step - loss: 3.5292 - acc: 0.0858 - val_loss: 3.5254 - val_acc: 0.1362\n",
            "Epoch 45/200\n",
            "86/86 [==============================] - 39s 447ms/step - loss: 3.5285 - acc: 0.0922 - val_loss: 3.5256 - val_acc: 0.1303\n",
            "Epoch 46/200\n",
            "86/86 [==============================] - 39s 447ms/step - loss: 3.5252 - acc: 0.0904 - val_loss: 3.5229 - val_acc: 0.1224\n",
            "Epoch 47/200\n",
            "86/86 [==============================] - 42s 487ms/step - loss: 3.5224 - acc: 0.0942 - val_loss: 3.5218 - val_acc: 0.1309\n",
            "Epoch 48/200\n",
            "86/86 [==============================] - 43s 502ms/step - loss: 3.5222 - acc: 0.0978 - val_loss: 3.5192 - val_acc: 0.0967\n",
            "Epoch 49/200\n",
            "86/86 [==============================] - 42s 483ms/step - loss: 3.5219 - acc: 0.0897 - val_loss: 3.5141 - val_acc: 0.1401\n",
            "Epoch 50/200\n",
            "86/86 [==============================] - 42s 489ms/step - loss: 3.5214 - acc: 0.0893 - val_loss: 3.5143 - val_acc: 0.1454\n",
            "Epoch 51/200\n",
            "86/86 [==============================] - 40s 466ms/step - loss: 3.5191 - acc: 0.0933 - val_loss: 3.5111 - val_acc: 0.1428\n",
            "Epoch 52/200\n",
            "86/86 [==============================] - 38s 446ms/step - loss: 3.5174 - acc: 0.0956 - val_loss: 3.5134 - val_acc: 0.1382\n",
            "Epoch 53/200\n",
            "86/86 [==============================] - 41s 469ms/step - loss: 3.5113 - acc: 0.0943 - val_loss: 3.5108 - val_acc: 0.1375\n",
            "Epoch 54/200\n",
            "86/86 [==============================] - 41s 478ms/step - loss: 3.5117 - acc: 0.0928 - val_loss: 3.5068 - val_acc: 0.1493\n",
            "Epoch 55/200\n",
            "86/86 [==============================] - 45s 519ms/step - loss: 3.5094 - acc: 0.0994 - val_loss: 3.5063 - val_acc: 0.1480\n",
            "Epoch 56/200\n",
            "86/86 [==============================] - 44s 503ms/step - loss: 3.5080 - acc: 0.0956 - val_loss: 3.5045 - val_acc: 0.1579\n",
            "Epoch 57/200\n",
            "86/86 [==============================] - 42s 489ms/step - loss: 3.5075 - acc: 0.0962 - val_loss: 3.5024 - val_acc: 0.1289\n",
            "Epoch 58/200\n",
            "86/86 [==============================] - 44s 513ms/step - loss: 3.5064 - acc: 0.0983 - val_loss: 3.5008 - val_acc: 0.1559\n",
            "Epoch 59/200\n",
            "86/86 [==============================] - 43s 491ms/step - loss: 3.5015 - acc: 0.0974 - val_loss: 3.4987 - val_acc: 0.1349\n",
            "Epoch 60/200\n",
            "86/86 [==============================] - 43s 500ms/step - loss: 3.5030 - acc: 0.1002 - val_loss: 3.4976 - val_acc: 0.1164\n",
            "Epoch 61/200\n",
            "86/86 [==============================] - 42s 481ms/step - loss: 3.5011 - acc: 0.1031 - val_loss: 3.4958 - val_acc: 0.1520\n",
            "Epoch 62/200\n",
            "86/86 [==============================] - 45s 523ms/step - loss: 3.4974 - acc: 0.0999 - val_loss: 3.4957 - val_acc: 0.1375\n",
            "Epoch 63/200\n",
            "86/86 [==============================] - 42s 485ms/step - loss: 3.4983 - acc: 0.1000 - val_loss: 3.4937 - val_acc: 0.1355\n",
            "Epoch 64/200\n",
            "86/86 [==============================] - 42s 483ms/step - loss: 3.4968 - acc: 0.1033 - val_loss: 3.4917 - val_acc: 0.1368\n",
            "Epoch 65/200\n",
            "86/86 [==============================] - 42s 480ms/step - loss: 3.4951 - acc: 0.1023 - val_loss: 3.4900 - val_acc: 0.1349\n",
            "Epoch 66/200\n",
            "86/86 [==============================] - 41s 475ms/step - loss: 3.4902 - acc: 0.1054 - val_loss: 3.4875 - val_acc: 0.1618\n",
            "Epoch 67/200\n",
            "86/86 [==============================] - 43s 494ms/step - loss: 3.4951 - acc: 0.1032 - val_loss: 3.4861 - val_acc: 0.1408\n",
            "Epoch 68/200\n",
            "86/86 [==============================] - 44s 507ms/step - loss: 3.4897 - acc: 0.1008 - val_loss: 3.4825 - val_acc: 0.1487\n",
            "Epoch 69/200\n",
            "86/86 [==============================] - 43s 495ms/step - loss: 3.4883 - acc: 0.1053 - val_loss: 3.4813 - val_acc: 0.1533\n",
            "Epoch 70/200\n",
            "86/86 [==============================] - 45s 523ms/step - loss: 3.4856 - acc: 0.1051 - val_loss: 3.4813 - val_acc: 0.1388\n",
            "Epoch 71/200\n",
            "86/86 [==============================] - 44s 510ms/step - loss: 3.4820 - acc: 0.1086 - val_loss: 3.4793 - val_acc: 0.1401\n",
            "Epoch 72/200\n",
            "86/86 [==============================] - 44s 512ms/step - loss: 3.4812 - acc: 0.1117 - val_loss: 3.4757 - val_acc: 0.1461\n",
            "Epoch 73/200\n",
            "86/86 [==============================] - 43s 501ms/step - loss: 3.4791 - acc: 0.1092 - val_loss: 3.4753 - val_acc: 0.1520\n",
            "Epoch 74/200\n",
            "86/86 [==============================] - 44s 505ms/step - loss: 3.4769 - acc: 0.1050 - val_loss: 3.4735 - val_acc: 0.1336\n",
            "Epoch 75/200\n",
            "86/86 [==============================] - 43s 501ms/step - loss: 3.4776 - acc: 0.1033 - val_loss: 3.4730 - val_acc: 0.1553\n",
            "Epoch 76/200\n",
            "86/86 [==============================] - 42s 489ms/step - loss: 3.4769 - acc: 0.1061 - val_loss: 3.4696 - val_acc: 0.1586\n",
            "Epoch 77/200\n",
            "86/86 [==============================] - 41s 473ms/step - loss: 3.4739 - acc: 0.1067 - val_loss: 3.4695 - val_acc: 0.1520\n",
            "Epoch 78/200\n",
            "86/86 [==============================] - 40s 463ms/step - loss: 3.4714 - acc: 0.1078 - val_loss: 3.4674 - val_acc: 0.1559\n",
            "Epoch 79/200\n",
            "86/86 [==============================] - 42s 479ms/step - loss: 3.4720 - acc: 0.1091 - val_loss: 3.4660 - val_acc: 0.1434\n",
            "Epoch 80/200\n",
            "86/86 [==============================] - 43s 491ms/step - loss: 3.4726 - acc: 0.1114 - val_loss: 3.4661 - val_acc: 0.1474\n",
            "Epoch 81/200\n",
            "86/86 [==============================] - 44s 505ms/step - loss: 3.4680 - acc: 0.1131 - val_loss: 3.4633 - val_acc: 0.1586\n",
            "Epoch 82/200\n",
            "86/86 [==============================] - 43s 494ms/step - loss: 3.4643 - acc: 0.1165 - val_loss: 3.4605 - val_acc: 0.1724\n",
            "Epoch 83/200\n",
            "86/86 [==============================] - 43s 497ms/step - loss: 3.4630 - acc: 0.1168 - val_loss: 3.4566 - val_acc: 0.1566\n",
            "Epoch 84/200\n",
            "86/86 [==============================] - 45s 516ms/step - loss: 3.4639 - acc: 0.1145 - val_loss: 3.4560 - val_acc: 0.1579\n",
            "Epoch 85/200\n",
            "86/86 [==============================] - 44s 505ms/step - loss: 3.4613 - acc: 0.1132 - val_loss: 3.4568 - val_acc: 0.1546\n",
            "Epoch 86/200\n",
            "86/86 [==============================] - 43s 495ms/step - loss: 3.4620 - acc: 0.1101 - val_loss: 3.4526 - val_acc: 0.1599\n",
            "Epoch 87/200\n",
            "86/86 [==============================] - 40s 459ms/step - loss: 3.4589 - acc: 0.1142 - val_loss: 3.4507 - val_acc: 0.1546\n",
            "Epoch 88/200\n",
            "86/86 [==============================] - 40s 465ms/step - loss: 3.4580 - acc: 0.1113 - val_loss: 3.4505 - val_acc: 0.1618\n",
            "Epoch 89/200\n",
            "86/86 [==============================] - 41s 476ms/step - loss: 3.4546 - acc: 0.1181 - val_loss: 3.4494 - val_acc: 0.1572\n",
            "Epoch 90/200\n",
            "86/86 [==============================] - 42s 490ms/step - loss: 3.4581 - acc: 0.1105 - val_loss: 3.4485 - val_acc: 0.1599\n",
            "Epoch 91/200\n",
            "86/86 [==============================] - 43s 498ms/step - loss: 3.4510 - acc: 0.1209 - val_loss: 3.4461 - val_acc: 0.1566\n",
            "Epoch 92/200\n",
            "86/86 [==============================] - 43s 500ms/step - loss: 3.4519 - acc: 0.1139 - val_loss: 3.4438 - val_acc: 0.1664\n",
            "Epoch 93/200\n",
            "86/86 [==============================] - 41s 472ms/step - loss: 3.4474 - acc: 0.1198 - val_loss: 3.4414 - val_acc: 0.1645\n",
            "Epoch 94/200\n",
            "86/86 [==============================] - 41s 474ms/step - loss: 3.4438 - acc: 0.1221 - val_loss: 3.4429 - val_acc: 0.1789\n",
            "Epoch 95/200\n",
            "86/86 [==============================] - 42s 481ms/step - loss: 3.4446 - acc: 0.1199 - val_loss: 3.4405 - val_acc: 0.1763\n",
            "Epoch 96/200\n",
            "86/86 [==============================] - 41s 474ms/step - loss: 3.4434 - acc: 0.1163 - val_loss: 3.4402 - val_acc: 0.1704\n",
            "Epoch 97/200\n",
            "86/86 [==============================] - 41s 472ms/step - loss: 3.4422 - acc: 0.1170 - val_loss: 3.4351 - val_acc: 0.1750\n",
            "Epoch 98/200\n",
            "86/86 [==============================] - 40s 461ms/step - loss: 3.4368 - acc: 0.1180 - val_loss: 3.4348 - val_acc: 0.1664\n",
            "Epoch 99/200\n",
            "86/86 [==============================] - 41s 471ms/step - loss: 3.4422 - acc: 0.1189 - val_loss: 3.4350 - val_acc: 0.1566\n",
            "Epoch 100/200\n",
            "86/86 [==============================] - 41s 474ms/step - loss: 3.4362 - acc: 0.1219 - val_loss: 3.4293 - val_acc: 0.1743\n",
            "Epoch 101/200\n",
            "86/86 [==============================] - 42s 489ms/step - loss: 3.4362 - acc: 0.1202 - val_loss: 3.4306 - val_acc: 0.1539\n",
            "Epoch 102/200\n",
            "86/86 [==============================] - 40s 460ms/step - loss: 3.4336 - acc: 0.1193 - val_loss: 3.4263 - val_acc: 0.1651\n",
            "Epoch 103/200\n",
            "86/86 [==============================] - 42s 480ms/step - loss: 3.4302 - acc: 0.1215 - val_loss: 3.4272 - val_acc: 0.1645\n",
            "Epoch 104/200\n",
            "86/86 [==============================] - 42s 487ms/step - loss: 3.4331 - acc: 0.1223 - val_loss: 3.4239 - val_acc: 0.1783\n",
            "Epoch 105/200\n",
            "86/86 [==============================] - 41s 477ms/step - loss: 3.4296 - acc: 0.1221 - val_loss: 3.4244 - val_acc: 0.1507\n",
            "Epoch 106/200\n",
            "86/86 [==============================] - 45s 516ms/step - loss: 3.4272 - acc: 0.1224 - val_loss: 3.4197 - val_acc: 0.1822\n",
            "Epoch 107/200\n",
            "86/86 [==============================] - 40s 461ms/step - loss: 3.4267 - acc: 0.1267 - val_loss: 3.4212 - val_acc: 0.1711\n",
            "Epoch 108/200\n",
            "86/86 [==============================] - 42s 483ms/step - loss: 3.4238 - acc: 0.1230 - val_loss: 3.4162 - val_acc: 0.1730\n",
            "Epoch 109/200\n",
            "86/86 [==============================] - 42s 482ms/step - loss: 3.4273 - acc: 0.1287 - val_loss: 3.4152 - val_acc: 0.1605\n",
            "Epoch 110/200\n",
            "86/86 [==============================] - 39s 453ms/step - loss: 3.4271 - acc: 0.1254 - val_loss: 3.4145 - val_acc: 0.1651\n",
            "Epoch 111/200\n",
            "86/86 [==============================] - 40s 466ms/step - loss: 3.4193 - acc: 0.1257 - val_loss: 3.4112 - val_acc: 0.1757\n",
            "Epoch 112/200\n",
            "86/86 [==============================] - 42s 480ms/step - loss: 3.4219 - acc: 0.1244 - val_loss: 3.4161 - val_acc: 0.1743\n",
            "Epoch 113/200\n",
            "86/86 [==============================] - 43s 493ms/step - loss: 3.4173 - acc: 0.1265 - val_loss: 3.4107 - val_acc: 0.1842\n",
            "Epoch 114/200\n",
            "86/86 [==============================] - 44s 506ms/step - loss: 3.4182 - acc: 0.1249 - val_loss: 3.4087 - val_acc: 0.1789\n",
            "Epoch 115/200\n",
            "86/86 [==============================] - 41s 470ms/step - loss: 3.4144 - acc: 0.1275 - val_loss: 3.4128 - val_acc: 0.1743\n",
            "Epoch 116/200\n",
            "86/86 [==============================] - 42s 488ms/step - loss: 3.4185 - acc: 0.1254 - val_loss: 3.4068 - val_acc: 0.1750\n",
            "Epoch 117/200\n",
            "86/86 [==============================] - 42s 485ms/step - loss: 3.4199 - acc: 0.1179 - val_loss: 3.4068 - val_acc: 0.1770\n",
            "Epoch 118/200\n",
            "86/86 [==============================] - 42s 480ms/step - loss: 3.4074 - acc: 0.1314 - val_loss: 3.4023 - val_acc: 0.1822\n",
            "Epoch 119/200\n",
            "86/86 [==============================] - 42s 482ms/step - loss: 3.4086 - acc: 0.1282 - val_loss: 3.4010 - val_acc: 0.1697\n",
            "Epoch 120/200\n",
            "86/86 [==============================] - 41s 470ms/step - loss: 3.4072 - acc: 0.1312 - val_loss: 3.3998 - val_acc: 0.1750\n",
            "Epoch 121/200\n",
            "86/86 [==============================] - 42s 487ms/step - loss: 3.4030 - acc: 0.1280 - val_loss: 3.3981 - val_acc: 0.1678\n",
            "Epoch 122/200\n",
            "86/86 [==============================] - 42s 485ms/step - loss: 3.4055 - acc: 0.1282 - val_loss: 3.3994 - val_acc: 0.1717\n",
            "Epoch 123/200\n",
            "86/86 [==============================] - 42s 483ms/step - loss: 3.4074 - acc: 0.1315 - val_loss: 3.3969 - val_acc: 0.1776\n",
            "Epoch 124/200\n",
            "86/86 [==============================] - 42s 481ms/step - loss: 3.4025 - acc: 0.1273 - val_loss: 3.3904 - val_acc: 0.1757\n",
            "Epoch 125/200\n",
            "86/86 [==============================] - 42s 490ms/step - loss: 3.4000 - acc: 0.1308 - val_loss: 3.3949 - val_acc: 0.1803\n",
            "Epoch 126/200\n",
            "86/86 [==============================] - 42s 484ms/step - loss: 3.3991 - acc: 0.1346 - val_loss: 3.3939 - val_acc: 0.1730\n",
            "Epoch 127/200\n",
            "86/86 [==============================] - 38s 443ms/step - loss: 3.3977 - acc: 0.1287 - val_loss: 3.3871 - val_acc: 0.1875\n",
            "Epoch 128/200\n",
            "86/86 [==============================] - 40s 456ms/step - loss: 3.3973 - acc: 0.1323 - val_loss: 3.3880 - val_acc: 0.1743\n",
            "Epoch 129/200\n",
            "86/86 [==============================] - 43s 493ms/step - loss: 3.3963 - acc: 0.1344 - val_loss: 3.3897 - val_acc: 0.1803\n",
            "Epoch 130/200\n",
            "86/86 [==============================] - 40s 458ms/step - loss: 3.3937 - acc: 0.1323 - val_loss: 3.3836 - val_acc: 0.1684\n",
            "Epoch 131/200\n",
            "86/86 [==============================] - 41s 476ms/step - loss: 3.3972 - acc: 0.1308 - val_loss: 3.3854 - val_acc: 0.1796\n",
            "Epoch 132/200\n",
            "86/86 [==============================] - 39s 445ms/step - loss: 3.3916 - acc: 0.1315 - val_loss: 3.3831 - val_acc: 0.1645\n",
            "Epoch 133/200\n",
            "86/86 [==============================] - 41s 468ms/step - loss: 3.3906 - acc: 0.1347 - val_loss: 3.3818 - val_acc: 0.1757\n",
            "Epoch 134/200\n",
            "86/86 [==============================] - 40s 463ms/step - loss: 3.3905 - acc: 0.1361 - val_loss: 3.3785 - val_acc: 0.1809\n",
            "Epoch 135/200\n",
            "86/86 [==============================] - 40s 462ms/step - loss: 3.3886 - acc: 0.1343 - val_loss: 3.3784 - val_acc: 0.1697\n",
            "Epoch 136/200\n",
            "86/86 [==============================] - 42s 484ms/step - loss: 3.3851 - acc: 0.1360 - val_loss: 3.3813 - val_acc: 0.1697\n",
            "Epoch 137/200\n",
            "86/86 [==============================] - 42s 488ms/step - loss: 3.3852 - acc: 0.1382 - val_loss: 3.3745 - val_acc: 0.1836\n",
            "Epoch 138/200\n",
            "86/86 [==============================] - 42s 489ms/step - loss: 3.3858 - acc: 0.1352 - val_loss: 3.3733 - val_acc: 0.1855\n",
            "Epoch 139/200\n",
            "86/86 [==============================] - 42s 482ms/step - loss: 3.3788 - acc: 0.1384 - val_loss: 3.3720 - val_acc: 0.1717\n",
            "Epoch 140/200\n",
            "86/86 [==============================] - 40s 463ms/step - loss: 3.3795 - acc: 0.1369 - val_loss: 3.3735 - val_acc: 0.1809\n",
            "Epoch 141/200\n",
            "86/86 [==============================] - 42s 485ms/step - loss: 3.3794 - acc: 0.1367 - val_loss: 3.3694 - val_acc: 0.1908\n",
            "Epoch 142/200\n",
            "86/86 [==============================] - 42s 484ms/step - loss: 3.3776 - acc: 0.1390 - val_loss: 3.3695 - val_acc: 0.1763\n",
            "Epoch 143/200\n",
            "86/86 [==============================] - 41s 475ms/step - loss: 3.3774 - acc: 0.1361 - val_loss: 3.3676 - val_acc: 0.1868\n",
            "Epoch 144/200\n",
            "86/86 [==============================] - 45s 518ms/step - loss: 3.3784 - acc: 0.1370 - val_loss: 3.3653 - val_acc: 0.1783\n",
            "Epoch 145/200\n",
            "86/86 [==============================] - 42s 488ms/step - loss: 3.3721 - acc: 0.1428 - val_loss: 3.3612 - val_acc: 0.1803\n",
            "Epoch 146/200\n",
            "86/86 [==============================] - 41s 474ms/step - loss: 3.3716 - acc: 0.1394 - val_loss: 3.3622 - val_acc: 0.1862\n",
            "Epoch 147/200\n",
            "86/86 [==============================] - 43s 494ms/step - loss: 3.3702 - acc: 0.1384 - val_loss: 3.3623 - val_acc: 0.2059\n",
            "Epoch 148/200\n",
            "86/86 [==============================] - 42s 484ms/step - loss: 3.3670 - acc: 0.1409 - val_loss: 3.3601 - val_acc: 0.1934\n",
            "Epoch 149/200\n",
            "86/86 [==============================] - 41s 475ms/step - loss: 3.3677 - acc: 0.1377 - val_loss: 3.3588 - val_acc: 0.1875\n",
            "Epoch 150/200\n",
            "86/86 [==============================] - 42s 487ms/step - loss: 3.3663 - acc: 0.1446 - val_loss: 3.3594 - val_acc: 0.1836\n",
            "Epoch 151/200\n",
            "86/86 [==============================] - 44s 508ms/step - loss: 3.3659 - acc: 0.1435 - val_loss: 3.3541 - val_acc: 0.1908\n",
            "Epoch 152/200\n",
            "86/86 [==============================] - 41s 472ms/step - loss: 3.3656 - acc: 0.1376 - val_loss: 3.3536 - val_acc: 0.1816\n",
            "Epoch 153/200\n",
            "86/86 [==============================] - 43s 502ms/step - loss: 3.3588 - acc: 0.1423 - val_loss: 3.3520 - val_acc: 0.1875\n",
            "Epoch 154/200\n",
            "86/86 [==============================] - 41s 477ms/step - loss: 3.3608 - acc: 0.1396 - val_loss: 3.3513 - val_acc: 0.1842\n",
            "Epoch 155/200\n",
            "86/86 [==============================] - 42s 483ms/step - loss: 3.3553 - acc: 0.1437 - val_loss: 3.3493 - val_acc: 0.1888\n",
            "Epoch 156/200\n",
            "86/86 [==============================] - 43s 496ms/step - loss: 3.3563 - acc: 0.1440 - val_loss: 3.3477 - val_acc: 0.1921\n",
            "Epoch 157/200\n",
            "86/86 [==============================] - 43s 492ms/step - loss: 3.3564 - acc: 0.1438 - val_loss: 3.3498 - val_acc: 0.1862\n",
            "Epoch 158/200\n",
            "86/86 [==============================] - 43s 493ms/step - loss: 3.3557 - acc: 0.1486 - val_loss: 3.3446 - val_acc: 0.1947\n",
            "Epoch 159/200\n",
            "86/86 [==============================] - 44s 508ms/step - loss: 3.3538 - acc: 0.1402 - val_loss: 3.3450 - val_acc: 0.1855\n",
            "Epoch 160/200\n",
            "86/86 [==============================] - 40s 465ms/step - loss: 3.3508 - acc: 0.1407 - val_loss: 3.3452 - val_acc: 0.1947\n",
            "Epoch 161/200\n",
            "86/86 [==============================] - 42s 479ms/step - loss: 3.3556 - acc: 0.1401 - val_loss: 3.3411 - val_acc: 0.2013\n",
            "Epoch 162/200\n",
            "86/86 [==============================] - 40s 465ms/step - loss: 3.3448 - acc: 0.1511 - val_loss: 3.3376 - val_acc: 0.1967\n",
            "Epoch 163/200\n",
            "86/86 [==============================] - 41s 474ms/step - loss: 3.3453 - acc: 0.1467 - val_loss: 3.3362 - val_acc: 0.1908\n",
            "Epoch 164/200\n",
            "86/86 [==============================] - 42s 479ms/step - loss: 3.3460 - acc: 0.1494 - val_loss: 3.3367 - val_acc: 0.1888\n",
            "Epoch 165/200\n",
            "86/86 [==============================] - 40s 462ms/step - loss: 3.3446 - acc: 0.1514 - val_loss: 3.3385 - val_acc: 0.2013\n",
            "Epoch 166/200\n",
            "86/86 [==============================] - 43s 496ms/step - loss: 3.3433 - acc: 0.1450 - val_loss: 3.3333 - val_acc: 0.1875\n",
            "Epoch 167/200\n",
            "86/86 [==============================] - 42s 486ms/step - loss: 3.3420 - acc: 0.1445 - val_loss: 3.3345 - val_acc: 0.1980\n",
            "Epoch 168/200\n",
            "86/86 [==============================] - 38s 441ms/step - loss: 3.3441 - acc: 0.1433 - val_loss: 3.3329 - val_acc: 0.1901\n",
            "Epoch 169/200\n",
            "86/86 [==============================] - 41s 469ms/step - loss: 3.3388 - acc: 0.1477 - val_loss: 3.3306 - val_acc: 0.1974\n",
            "Epoch 170/200\n",
            "86/86 [==============================] - 39s 453ms/step - loss: 3.3380 - acc: 0.1463 - val_loss: 3.3299 - val_acc: 0.2007\n",
            "Epoch 171/200\n",
            "86/86 [==============================] - 41s 468ms/step - loss: 3.3396 - acc: 0.1448 - val_loss: 3.3295 - val_acc: 0.2000\n",
            "Epoch 172/200\n",
            "86/86 [==============================] - 41s 477ms/step - loss: 3.3370 - acc: 0.1472 - val_loss: 3.3259 - val_acc: 0.2046\n",
            "Epoch 173/200\n",
            "86/86 [==============================] - 42s 488ms/step - loss: 3.3351 - acc: 0.1483 - val_loss: 3.3247 - val_acc: 0.1908\n",
            "Epoch 174/200\n",
            "86/86 [==============================] - 42s 480ms/step - loss: 3.3353 - acc: 0.1489 - val_loss: 3.3254 - val_acc: 0.1888\n",
            "Epoch 175/200\n",
            "86/86 [==============================] - 41s 477ms/step - loss: 3.3364 - acc: 0.1456 - val_loss: 3.3228 - val_acc: 0.2000\n",
            "Epoch 176/200\n",
            "86/86 [==============================] - 41s 477ms/step - loss: 3.3314 - acc: 0.1494 - val_loss: 3.3196 - val_acc: 0.1941\n",
            "Epoch 177/200\n",
            "86/86 [==============================] - 40s 465ms/step - loss: 3.3276 - acc: 0.1485 - val_loss: 3.3207 - val_acc: 0.1967\n",
            "Epoch 178/200\n",
            "86/86 [==============================] - 41s 477ms/step - loss: 3.3293 - acc: 0.1497 - val_loss: 3.3158 - val_acc: 0.1888\n",
            "Epoch 179/200\n",
            "86/86 [==============================] - 40s 465ms/step - loss: 3.3266 - acc: 0.1553 - val_loss: 3.3189 - val_acc: 0.1934\n",
            "Epoch 180/200\n",
            "86/86 [==============================] - 40s 464ms/step - loss: 3.3275 - acc: 0.1526 - val_loss: 3.3133 - val_acc: 0.2000\n",
            "Epoch 181/200\n",
            "86/86 [==============================] - 42s 490ms/step - loss: 3.3211 - acc: 0.1501 - val_loss: 3.3119 - val_acc: 0.2151\n",
            "Epoch 182/200\n",
            "86/86 [==============================] - 39s 455ms/step - loss: 3.3201 - acc: 0.1549 - val_loss: 3.3111 - val_acc: 0.2066\n",
            "Epoch 183/200\n",
            "86/86 [==============================] - 41s 474ms/step - loss: 3.3232 - acc: 0.1592 - val_loss: 3.3106 - val_acc: 0.1980\n",
            "Epoch 184/200\n",
            "86/86 [==============================] - 42s 489ms/step - loss: 3.3195 - acc: 0.1535 - val_loss: 3.3093 - val_acc: 0.2086\n",
            "Epoch 185/200\n",
            "86/86 [==============================] - 42s 480ms/step - loss: 3.3122 - acc: 0.1576 - val_loss: 3.3056 - val_acc: 0.2026\n",
            "Epoch 186/200\n",
            "86/86 [==============================] - 42s 484ms/step - loss: 3.3179 - acc: 0.1516 - val_loss: 3.3081 - val_acc: 0.1868\n",
            "Epoch 187/200\n",
            "86/86 [==============================] - 40s 460ms/step - loss: 3.3146 - acc: 0.1549 - val_loss: 3.3048 - val_acc: 0.1947\n",
            "Epoch 188/200\n",
            "86/86 [==============================] - 40s 462ms/step - loss: 3.3161 - acc: 0.1541 - val_loss: 3.3053 - val_acc: 0.1875\n",
            "Epoch 189/200\n",
            "86/86 [==============================] - 44s 504ms/step - loss: 3.3170 - acc: 0.1567 - val_loss: 3.3043 - val_acc: 0.2000\n",
            "Epoch 190/200\n",
            "36/86 [===========>..................] - ETA: 20s - loss: 3.3122 - acc: 0.1560"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_1mVhsrMCDC"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "model_json = model.to_json()\n",
        "with open(\"model_resnet.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model_resnet.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtiTvkJZMYgs"
      },
      "source": [
        "# Utility function for plotting of the model results\n",
        "def visualize_results(history):\n",
        "    # Plot the accuracy and loss curves\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Run the function to illustrate accuracy and loss\n",
        "visualize_results(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1rD4nNiMeTx"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "num_of_test_samples = 1520\n",
        "batch_size = 100\n",
        "Y_pred = loaded_model.predict_generator(validation_generator, num_of_test_samples // batch_size+1)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(validation_generator.classes, y_pred))\n",
        "print('Classification Report')\n",
        "target_names = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37',]\n",
        "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}